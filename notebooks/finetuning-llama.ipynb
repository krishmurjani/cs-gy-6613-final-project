{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10023826,"sourceType":"datasetVersion","datasetId":6172714},{"sourceId":10137247,"sourceType":"datasetVersion","datasetId":6256422},{"sourceId":120000,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":100931,"modelId":121027}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers \n!pip install accelerate \n!pip install bitsandbytes\n!pip install peft\n!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T23:17:34.522577Z","iopub.execute_input":"2024-12-08T23:17:34.523455Z","iopub.status.idle":"2024-12-08T23:18:15.382900Z","shell.execute_reply.started":"2024-12-08T23:17:34.523412Z","shell.execute_reply":"2024-12-08T23:18:15.381793Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.1.1)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.26.2)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.45.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig, \n    DataCollatorForLanguageModeling, \n    Trainer, \n    TrainingArguments\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wanb-api-key\")\n\n\n# step 1: using llama 3.2 \nbase_model = \"/kaggle/input/llama-3.2/transformers/1b/1\"\n\ndf = pd.read_csv('/kaggle/input/questions/questions.csv')\ndf = df.iloc[:, :2]\n\ndef format_example(row):\n    return f\"Question: {row['Question']}\\nAnswer: {row['Answer']}\"\n\ndf['text'] = df.apply(format_example, axis=1)\ndataset = Dataset.from_pandas(df[['text']])\n\n# step 2: loading the tokenizer for llama 3.2\ntokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# step 3: quantization config using bitsandbytes\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True\n)\n\n# step 4: loading llama 3.2 model \nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    return_dict=True,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    quantization_config=bnb_config\n)\n\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# tokenzing dataset\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        padding='max_length',\n        truncation=True,\n        max_length=512,\n        return_tensors='pt'\n    )\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# step 5: lora config\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# step 6: training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=10,  # reduced from 100 because of gpu constraints\n    per_device_train_batch_size=8,  \n    gradient_accumulation_steps=2, \n    learning_rate=5e-4,  \n    fp16=True,  \n    logging_steps=10,\n    save_steps=100,  \n    save_total_limit=2,\n    evaluation_strategy='no',\n    optim=\"paged_adamw_8bit\",\n    ddp_find_unused_parameters=False\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\n# step 7: train \ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator\n)\n\ntrainer.train()\n\n# step 8: save the peft model\nmodel.save_pretrained('./finetuned_llama')\ntokenizer.save_pretrained('./finetuned_llama')\n\n# step 9: load the finetuned model and generate answers\nbase_model_instance = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    device_map='auto',\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n    torch_dtype=torch.float16\n)\n\n# loading the tokenizer and lora-adapted model\ntokenizer = AutoTokenizer.from_pretrained('./finetuned_llama', use_fast=False)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = PeftModel.from_pretrained(base_model_instance, './finetuned_llama')\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\ndef generate_answer(question, max_length=200):\n    input_text = f\"Question: {question}\\nAnswer:\"\n    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n    output = model.generate(\n        input_ids,\n        max_length=max_length,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,\n        early_stopping=True,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n    # Extract the answer part\n    if \"Answer:\" in answer:\n        answer = answer.split(\"Answer:\")[1].strip()\n    return answer\n\n# trying an example \nquestion = \"How can I implement obstacle avoidance in ROS2 using Nav2?\"\nanswer = generate_answer(question)\nprint(\"Question:\", question)\nprint(\"Answer:\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T23:23:41.353188Z","iopub.execute_input":"2024-12-08T23:23:41.353598Z","iopub.status.idle":"2024-12-09T01:20:39.285762Z","shell.execute_reply.started":"2024-12-08T23:23:41.353529Z","shell.execute_reply":"2024-12-09T01:20:39.284624Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3258 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"767bf12af6af4264b8b360dec998639d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2040' max='2040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2040/2040 1:56:38, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.345100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.089700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.961400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.993300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.950600</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.927000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.818000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.863100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.947700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.826500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.851600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.762000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.823900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.730700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.717000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.785700</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.695200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.658400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.677500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.711800</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.468900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.271300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.285800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.263400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.311900</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.268900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.257600</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.299600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.229200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.304900</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.292700</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.276800</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.249900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.279500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.292900</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.230400</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.291000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.272700</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.267800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.321400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.177400</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.804200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.828000</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.873400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.784200</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.802700</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.835400</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.828600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.860800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.822900</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.858000</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.876600</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.901100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.878000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.924600</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.880200</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.859900</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.909200</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.840700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.912400</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.894500</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.628900</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.568100</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.534600</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.564600</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.562400</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.574700</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.592200</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.601400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.601000</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.589400</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.612000</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.607700</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.601200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.614600</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.592900</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.658300</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.575600</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.617600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.628300</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.624100</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.528700</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.421300</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.429600</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.431900</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.425300</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.437100</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.424000</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.462400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.445200</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.455900</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.449400</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.455600</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.467700</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.432400</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.462600</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.473300</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.464700</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.486800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.494800</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.481100</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.491200</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.356800</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.352100</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.356000</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.380400</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.358900</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.359800</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.354800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.367500</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.358800</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.369200</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.366100</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.386500</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.378100</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.361100</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.388900</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.392200</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.380000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.395200</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.384700</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.381600</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.340200</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.300900</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.318200</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.307700</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.303400</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.312200</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.324500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.319000</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.298000</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.320400</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.334600</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.326400</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.330100</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.315400</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.329100</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.317600</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.317700</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.320400</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.338700</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.327300</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.309900</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.289900</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.265100</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.260600</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.286000</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.272900</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.285900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.264600</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.298900</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.269400</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.277200</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.267700</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.289900</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.287500</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.302700</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.291200</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.277500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.297400</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.291100</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.276000</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.291800</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.247700</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.243300</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.241300</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.239500</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.249100</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.236600</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.243200</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.254400</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.263600</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.276400</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.242800</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.246800</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.257500</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.248200</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.268000</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.257200</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.264100</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.255100</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.252500</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.255900</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.236900</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.230600</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.224000</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.224800</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.234200</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.237600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.231500</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.234000</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.237900</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.213800</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.224800</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.231500</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.237900</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.231900</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.230800</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.225800</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.227900</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.232900</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.242700</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.230100</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.224000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Question: How can I implement obstacle avoidance in ROS2 using Nav2?\nAnswer: \"Nav2 provides distance and velocity sensors for obstacle detection. I would recommend integrating these sensors with ROS 2's obstacle avoiding module and adding physical properties like the size of the obstacle.\" This can be done by configuring the robot state representation and integrating the distance sensor data. Then, you can avoid obstacles using the same algorithm as inROS 1.\"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"question = \"Tell me how can I navigate to a specific pose - include replanning aspects in your answer.\"\nanswer = generate_answer(question)\nprint(\"Question:\", question)\nprint(\"Answer:\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T01:22:12.471442Z","iopub.execute_input":"2024-12-09T01:22:12.471827Z","iopub.status.idle":"2024-12-09T01:22:17.681131Z","shell.execute_reply.started":"2024-12-09T01:22:12.471798Z","shell.execute_reply":"2024-12-09T01:22:17.680123Z"}},"outputs":[{"name":"stdout","text":"Question: Tell me how can I navigate to a specific pose - include replanning aspects in your answer.\nAnswer: To navigate specifically toa pose, you can use the command: ros2 pose --ros-args --params-file params.yaml --node-name my_node /pose:=/pose'\". This will direct the robot to follow a particular pose.\" For more detailed navigation instructions, refer to the navigation2 example and parameters.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"question = \"Tell me how can I navigate to a specific pose - include replanning aspects in your answer. Can you provide me with code for this task?\"\nanswer = generate_answer(question)\nprint(\"Question:\", question)\nprint(\"Answer:\", answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T01:35:27.205610Z","iopub.execute_input":"2024-12-09T01:35:27.206484Z","iopub.status.idle":"2024-12-09T01:35:40.648736Z","shell.execute_reply.started":"2024-12-09T01:35:27.206444Z","shell.execute_reply":"2024-12-09T01:35:40.646376Z"}},"outputs":[{"name":"stdout","text":"Question: Tell me how can I navigate to a specific pose - include replanning aspects in your answer. Can you provide me with code for this task?\nAnswer: Yes, you can use the 'ros2 pose' tool to set a goal pose and manage replanation details. For example, use 'ROS2 Pose --goal 2 3 0' to adjust a target pose. Additionally, check out the demo from the ROS 2024 ROS Core repository for detailed guidance on navigating to targets. It includes both pure ROS and Gazebo-based solutions. Finally, if you want to plan a route to reach a pose, consider using the \"ros3 route\" tool. These tools make it easier to manage complex movement tasks in ROS. You can find more examples and detailed instructions in the repository's documentation. I hope this helps with navigation inROS. Let me know if I missed any crucial features. Thank you for including me in this community!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# # pushing to hf\nimport os\nfrom huggingface_hub import HfApi\n\nos.environ['hugging_face_token'] = ' '\napi = HfApi(token=os.environ['hugging_face_token'])\n\nuser_info = api.whoami()\nuser = user_info['name']\nprint(f\"Logged in as: {user}\")\n\nrepo_name = \"finetuned-llama\"\nrepo_id = f\"{user}/{repo_name}\"\n\napi.create_repo(repo_id=repo_id, private=False, exist_ok=True)\n\nmodel.push_to_hub(repo_id, token=os.environ['hugging_face_token'])\ntokenizer.push_to_hub(repo_id, token=os.environ['hugging_face_token'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T01:41:29.567564Z","iopub.execute_input":"2024-12-09T01:41:29.568199Z","iopub.status.idle":"2024-12-09T01:41:36.041456Z","shell.execute_reply.started":"2024-12-09T01:41:29.568154Z","shell.execute_reply":"2024-12-09T01:41:36.040595Z"}},"outputs":[{"name":"stdout","text":"Logged in as: krishmurjani\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/45.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"279bb602fb114aa3bac0950c8ed242bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4e5db8f21d149a7b25c642989b51f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"514db83121434ac196cdcbeee043b531"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/krishmurjani/finetuned-llama/commit/3a17b409a725a1026bcae8c5aad0f239d98d3b7f', commit_message='Upload tokenizer', commit_description='', oid='3a17b409a725a1026bcae8c5aad0f239d98d3b7f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/krishmurjani/finetuned-llama', endpoint='https://huggingface.co', repo_type='model', repo_id='krishmurjani/finetuned-llama'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":22}]}
